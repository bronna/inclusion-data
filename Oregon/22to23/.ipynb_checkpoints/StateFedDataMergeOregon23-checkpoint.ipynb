{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191673af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briannagreen/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import topojson as tp\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4846a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the TopoJSON data\n",
    "with open(\"OR_SDs_simplified_23.topojson\", \"r\") as file: topo_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3283be",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SpEdMediaDistrict.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-aa4509136df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the state data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SpEdMediaDistrict.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SpEdMediaDistrict.csv'"
     ]
    }
   ],
   "source": [
    "# load the state data\n",
    "state_data = pd.read_csv(\"SpEdMediaDistrict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique names from TopoJSON data\n",
    "names_topojson = [feature['properties']['NAME'] for feature in topo_data['objects']['OR_SDs_merged']['geometries']]\n",
    "print(names_topojson[:10])  # First 10 names for a glimpse\n",
    "\n",
    "# Check unique institution names from state data\n",
    "print(state_data['Institution Name'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4079de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each feature in the geometries of topo_data\n",
    "for feature in topo_data['objects']['OR_SDs_merged']['geometries']:\n",
    "    # Check if 'NAME' exists in properties\n",
    "    if 'NAME' in feature['properties']:\n",
    "        # Replace \"School District\" with an empty string\n",
    "        name = feature['properties']['NAME'].replace(\"School District\", \"\").strip()\n",
    "        # Replace any sequence of spaces with a single space\n",
    "        name = re.sub(' +', ' ', name)\n",
    "        feature['properties']['NAME'] = name\n",
    "\n",
    "        # Check unique names from TopoJSON data\n",
    "names_topojson = [feature['properties']['NAME'] for feature in topo_data['objects']['OR_SDs_merged']['geometries']]\n",
    "print(names_topojson[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8588d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"SD\" from state data\n",
    "state_data['Institution Name'] = state_data['Institution Name'].str.replace(\" SD\", \"\", case=False)\n",
    "\n",
    "# Check unique institution names from state data\n",
    "print(state_data['Institution Name'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the relevant portion of topo_data to a DataFrame\n",
    "topo_df = pd.DataFrame([{\n",
    "    'NAME': feature['properties']['NAME'],\n",
    "    'geometry_index': index  # Store the index to later update topo_data\n",
    "} for index, feature in enumerate(topo_data['objects']['OR_SDs_merged']['geometries'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with state_data\n",
    "merged_df = topo_df.merge(state_data, left_on='NAME', right_on='Institution Name', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find non-matching entries in both datasets and print first few columns\n",
    "\n",
    "non_matching_topojson = merged_df[merged_df['Institution Name'].isna()]\n",
    "\n",
    "print(\"Non-matching from TopoJSON:\", non_matching_topojson.iloc[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_state = merged_df[merged_df['NAME'].isna()]\n",
    "\n",
    "print(\"Non-matching from state data:\", non_matching_state.iloc[:, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e87fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually match unmatched entries\n",
    "\n",
    "# Yamhill Carlton 1\n",
    "# Find the rows to match\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Yamhill-Carlton 1']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Yamhill Carlton 1']\n",
    "\n",
    "# Combine the data\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "# Drop the unmatched row\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Ione R2\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Ione 2']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Ione R2']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# North Wasco County 21\n",
    "row_topo = merged_df[merged_df['NAME'] == 'North Wasco 21']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'North Wasco County 21']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Greater Albany Public 8J\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Greater Albany 8J']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Greater Albany Public 8J']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Athena-Weston 29RJ\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Athena-Weston 29J']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Athena-Weston 29RJ']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Bend-LaPine Administrative 1\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Bend-La Pine Administrative 1']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Bend-LaPine Administrative 1']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Blachly 90\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Blachly 090']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Blachly 90']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Brookings-Harbor 17C\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Brookings-Harbor 17']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Brookings-Harbor 17C']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Crow-Applegate-Lorane 66\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Crow-Applegate-Lorane Sd 66']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Crow-Applegate-Lorane 66']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Gresham-Barlow 10J\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Gresham-Barlow 1J']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Gresham-Barlow 10J']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Hood River County\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Hood River County 1']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Hood River County']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Three Rivers/Josephine County\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Three Rivers']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Three Rivers/Josephine County']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Lake County 7\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Lakeview 7']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Lake County 7']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Medford 549C\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Medford 549']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Medford 549C']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Milton-Freewater Unified 7\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Milton-Freewater 7']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Milton-Freewater Unified 7']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Mt Angel 91\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Mount Angel 91']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Mt Angel 91']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Ontario 8C\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Ontario 8']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Ontario 8C']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Pine Eagle 61\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Pine-Eagle 61']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Pine Eagle 61']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Port Orford-Langlois 2CJ\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Port Orford-Langlois 2J']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Port Orford-Langlois 2CJ']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Sherman County\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Sherman 1']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Sherman County']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# South Lane 45J3\n",
    "row_topo = merged_df[merged_df['NAME'] == 'South Lane 45J']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'South Lane 45J3']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# St Helens 502\n",
    "row_topo = merged_df[merged_df['NAME'] == 'St. Helens 502']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'St Helens 502']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# St Paul 45\n",
    "row_topo = merged_df[merged_df['NAME'] == 'St. Paul 45']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'St Paul 45']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Ukiah 80R\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Ukiah 80']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Ukiah 80R']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Umatilla 6R\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Umatilla 6']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Umatilla 6R']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# West Linn-Wilsonville 3J\n",
    "row_topo = merged_df[merged_df['NAME'] == 'West Linn 3J']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'West Linn-Wilsonville 3J']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Douglas County 15\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Days Creek 15']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Douglas County 15']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Douglas County 4\n",
    "row_topo = merged_df[merged_df['NAME'] == 'Roseburg 4']\n",
    "row_state = merged_df[merged_df['Institution Name'] == 'Douglas County 4']\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if pd.isna(row_topo[column].iloc[0]) and not pd.isna(row_state[column].iloc[0]):\n",
    "        merged_df.loc[row_topo.index, column] = row_state[column].iloc[0]\n",
    "\n",
    "merged_df.drop(row_state.index, inplace=True)\n",
    "\n",
    "\n",
    "# Reset index\n",
    "merged_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fa626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find remaining non-matching entries\n",
    "\n",
    "non_matching_topojson = merged_df[merged_df['Institution Name'].isna()]\n",
    "\n",
    "print(\"Non-matching from TopoJSON:\", non_matching_topojson.iloc[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_state = merged_df[merged_df['NAME'].isna()]\n",
    "\n",
    "print(\"Non-matching from state data:\", non_matching_state.iloc[:, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric type\n",
    "cols_to_convert = ['Total Student Count', 'LRE Students >80%', 'LRE Students <40%', 'LRE Students Separate Settings', 'IEP 4Yr Cohort Grad 18-19', 'IEP Dropout 18-19', 'Higher Ed/Training/Employed']\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    # Replace '%' and '>' signs with an empty string\n",
    "    merged_df[col] = pd.to_numeric(merged_df[col].str.replace('[%><]', '', regex=True), errors='coerce')\n",
    "\n",
    "# # Replace any NaN values with NaN\n",
    "# merged_df[cols_to_convert] = merged_df[cols_to_convert].fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd996bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for percent of students who are in regular class >40% and <80%\n",
    "merged_df['LRE Students >40% <80%'] = 100 - merged_df['LRE Students >80%'] - merged_df['LRE Students <40%'] - merged_df['LRE Students Separate Settings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim trailing spaces off of alerts columns\n",
    "# Columns to trim\n",
    "cols_to_trim = [\n",
    "    'SuspExplFg',\n",
    "    'SuspExplRaceEthnicityFg',\n",
    "    'DisPrptnRprsntnFg',\n",
    "    'DisPrptnRprsntnDsbltyFg'\n",
    "]\n",
    "\n",
    "# Trim trailing spaces for the specified columns\n",
    "merged_df[cols_to_trim] = merged_df[cols_to_trim].apply(lambda x: x.str.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"ODE YCEP District\" since there are no students or geographies recorded\n",
    "merged_df = merged_df.dropna(subset=['geometry_index'])\n",
    "\n",
    "# Drop other NaN rows\n",
    "merged_df = merged_df.dropna(subset=['Institution ID'])\n",
    "\n",
    "# Convert the geometry_index column to integers instead of float\n",
    "merged_df.loc[:, 'geometry_index'] = merged_df['geometry_index'].astype(int)\n",
    "\n",
    "# Convert '*' values to \"null\"\n",
    "merged_df.replace('*', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792715df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the topo_data with the merged data\n",
    "for _, row in merged_df.iterrows():\n",
    "    # Fetch the geometry using the stored index\n",
    "    geometry = topo_data['objects']['OR_SDs_merged']['geometries'][row['geometry_index']]\n",
    "    # Update properties with the merged data\n",
    "    # Assuming state_data has columns 'Col1', 'Col2', etc. which you want to add to topo_data\n",
    "    geometry['properties'].update({\n",
    "        'Institution Name': row['Institution Name'],\n",
    "        'Total Student Count': row['Total Student Count'],\n",
    "        'LRE Students >80%': row['LRE Students >80%'],\n",
    "        'LRE Students >40% <80%': row['LRE Students >40% <80%'],\n",
    "        'LRE Students <40%': row['LRE Students <40%'],\n",
    "        'LRE Students Separate Settings': row['LRE Students Separate Settings'],\n",
    "        'SuspExplFg': row['SuspExplFg'],\n",
    "        'SuspExplRaceEthnicityFg': row['SuspExplRaceEthnicityFg'],\n",
    "        'DisPrptnRprsntnFg': row['DisPrptnRprsntnFg'],\n",
    "        'DisPrptnRprsntnDsbltyFg': row['DisPrptnRprsntnDsbltyFg'],\n",
    "        'IEP 4Yr Cohort Grad 18-19': row['IEP 4Yr Cohort Grad 18-19'], \n",
    "        'IEP Dropout 18-19': row['IEP Dropout 18-19'], \n",
    "        'Higher Ed/Training/Employed': row['Higher Ed/Training/Employed'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort district geometries & data by alphabetical order\n",
    "sorted_geometries = sorted(topo_data['objects']['OR_SDs_merged']['geometries'], \n",
    "                           key=lambda x: x['properties'].get('Institution Name', ''))\n",
    "\n",
    "# Assign the sorted geometries back to the topo_data\n",
    "topo_data['objects']['OR_SDs_merged']['geometries'] = sorted_geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e72c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all instances of NaN to None\n",
    "def convert_nan(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_nan(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_nan(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return None if obj is np.nan or str(obj).lower() == 'nan' else obj\n",
    "    \n",
    "converted_data = convert_nan(topo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(converted_data['objects']['OR_SDs_merged']['geometries'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282876d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(converted_data[\"objects\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731eaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to output file\n",
    "output_file_path = \"oregon_data.topojson\"\n",
    "\n",
    "# Export\n",
    "with open(output_file_path, 'w') as f:\n",
    "    json.dump(converted_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1fb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
